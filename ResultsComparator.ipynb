{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResultsComparator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-WMqJyft9o-",
        "outputId": "a7beac8a-0390-4d34-b44a-1d4b1d0bc604"
      },
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB2a5Lwpj-Ds",
        "outputId": "ba405632-bcaf-47cb-e4d6-7238e0708cbd"
      },
      "source": [
        "!pip install jsonpickle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle) (3.4.1)\n",
            "Installing collected packages: jsonpickle\n",
            "Successfully installed jsonpickle-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuxjJNGIufZ7"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import jsonpickle \n",
        "import math\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import numpy as np # linear algebra\n",
        "from itertools import combinations \n",
        "from collections import defaultdict \n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize\n",
        "from scipy import spatial\n",
        "from nltk.metrics import edit_distance\n",
        "from collections import defaultdict "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9XDTvQNiu-j",
        "outputId": "50ab68ad-bb7f-4ff7-d09c-b04a23e77a27"
      },
      "source": [
        "nltk.download('stopwords')  \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljptIv-Viw9d"
      },
      "source": [
        "def tokenize(q1, q2):\n",
        "    \"\"\"\n",
        "        q1 and q2 are sentences/questions. Function returns a list of tokens for both.\n",
        "    \"\"\"\n",
        "    return word_tokenize(q1), word_tokenize(q2)\n",
        "\n",
        "\n",
        "def posTag(q1, q2):\n",
        "    \"\"\"\n",
        "        q1 and q2 are lists. Function returns a list of POS tagged tokens for both.\n",
        "    \"\"\"\n",
        "    return nltk.pos_tag(q1), nltk.pos_tag(q2)\n",
        "\n",
        "\n",
        "def stemmer(tag_q1, tag_q2):\n",
        "    \"\"\"\n",
        "        tag_q = tagged lists. Function returns a stemmed list.\n",
        "    \"\"\"\n",
        "\n",
        "    stem_q1 = []\n",
        "    stem_q2 = []\n",
        "\n",
        "    for token in tag_q1:\n",
        "        stem_q1.append(stem(token))\n",
        "\n",
        "    for token in tag_q2:\n",
        "        stem_q2.append(stem(token))\n",
        "\n",
        "    return stem_q1, stem_q2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI6ltuUfizPw"
      },
      "source": [
        "class Lesk(object):\n",
        "\n",
        "    def __init__(self, sentence):\n",
        "        self.sentence = sentence\n",
        "        self.meanings = {}\n",
        "        for word in sentence:\n",
        "            self.meanings[word] = ''\n",
        "\n",
        "    def getSenses(self, word):\n",
        "        # print word\n",
        "        return wn.synsets(word.lower())\n",
        "\n",
        "    def getGloss(self, senses):\n",
        "\n",
        "        gloss = {}\n",
        "\n",
        "        for sense in senses:\n",
        "            gloss[sense.name()] = []\n",
        "\n",
        "        for sense in senses:\n",
        "            gloss[sense.name()] += word_tokenize(sense.definition())\n",
        "\n",
        "        return gloss\n",
        "\n",
        "    def getAll(self, word):\n",
        "        senses = self.getSenses(word)\n",
        "\n",
        "        if senses == []:\n",
        "            return {word.lower(): senses}\n",
        "\n",
        "        return self.getGloss(senses)\n",
        "\n",
        "    def Score(self, set1, set2):\n",
        "        # Base\n",
        "        overlap = 0\n",
        "\n",
        "        # Step\n",
        "        for word in set1:\n",
        "            if word in set2:\n",
        "                overlap += 1\n",
        "\n",
        "        return overlap\n",
        "\n",
        "    def overlapScore(self, word1, word2):\n",
        "\n",
        "        gloss_set1 = self.getAll(word1)\n",
        "        if self.meanings[word2] == '':\n",
        "            gloss_set2 = self.getAll(word2)\n",
        "        else:\n",
        "            # print 'here'\n",
        "            gloss_set2 = self.getGloss([wn.synset(self.meanings[word2])])\n",
        "\n",
        "        # print gloss_set2\n",
        "\n",
        "        score = {}\n",
        "        for i in gloss_set1.keys():\n",
        "            score[i] = 0\n",
        "            for j in gloss_set2.keys():\n",
        "                score[i] += self.Score(gloss_set1[i], gloss_set2[j])\n",
        "\n",
        "        bestSense = None\n",
        "        max_score = 0\n",
        "        for i in gloss_set1.keys():\n",
        "            if score[i] > max_score:\n",
        "                max_score = score[i]\n",
        "                bestSense = i\n",
        "\n",
        "        return bestSense, max_score\n",
        "\n",
        "    def lesk(self, word, sentence):\n",
        "        maxOverlap = 0\n",
        "        context = sentence\n",
        "        word_sense = []\n",
        "        meaning = {}\n",
        "\n",
        "        senses = self.getSenses(word)\n",
        "\n",
        "        for sense in senses:\n",
        "            meaning[sense.name()] = 0\n",
        "\n",
        "        for word_context in context:\n",
        "            if not word == word_context:\n",
        "                score = self.overlapScore(word, word_context)\n",
        "                if score[0] == None:\n",
        "                    continue\n",
        "                meaning[score[0]] += score[1]\n",
        "\n",
        "        if senses == []:\n",
        "            return word, None, None\n",
        "\n",
        "        self.meanings[word] = max(meaning.keys(), key=lambda x: meaning[x])\n",
        "\n",
        "        return word, self.meanings[word], wn.synset(self.meanings[word]).definition()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQM6pRJii1H3"
      },
      "source": [
        "def path(set1, set2):\n",
        "    return wn.path_similarity(set1, set2)\n",
        "\n",
        "\n",
        "def wup(set1, set2):\n",
        "    return wn.wup_similarity(set1, set2)\n",
        "\n",
        "\n",
        "def edit(word1, word2):\n",
        "    if float(edit_distance(word1, word2)) == 0.0:\n",
        "        return 0.0\n",
        "    return 1.0 / float(edit_distance(word1, word2))\n",
        "\n",
        "def computePath(q1, q2):\n",
        "\n",
        "    R = np.zeros((len(q1), len(q2)))\n",
        "\n",
        "    for i in range(len(q1)):\n",
        "        for j in range(len(q2)):\n",
        "            if q1[i][1] == None or q2[j][1] == None:\n",
        "                sim = edit(q1[i][0], q2[j][0])\n",
        "            else:\n",
        "                sim = path(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
        "\n",
        "            if sim == None:\n",
        "                sim = edit(q1[i][0], q2[j][0])\n",
        "\n",
        "            R[i, j] = sim\n",
        "\n",
        "    # print R\n",
        "\n",
        "    return R\n",
        "\n",
        "def computeWup(q1, q2):\n",
        "\n",
        "    R = np.zeros((len(q1), len(q2)))\n",
        "\n",
        "    for i in range(len(q1)):\n",
        "        for j in range(len(q2)):\n",
        "            if q1[i][1] == None or q2[j][1] == None:\n",
        "                sim = edit(q1[i][0], q2[j][0])\n",
        "            else:\n",
        "                sim = wup(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
        "\n",
        "            if sim == None:\n",
        "                sim = edit(q1[i][0], q2[j][0])\n",
        "\n",
        "            R[i, j] = sim\n",
        "\n",
        "    # print R\n",
        "\n",
        "    return R\n",
        "\n",
        "def overallSim(q1, q2, R):\n",
        "\n",
        "    sum_X = 0.0\n",
        "    sum_Y = 0.0\n",
        "\n",
        "    for i in range(len(q1)):\n",
        "        max_i = 0.0\n",
        "        for j in range(len(q2)):\n",
        "            if R[i, j] > max_i:\n",
        "                max_i = R[i, j]\n",
        "        sum_X += max_i\n",
        "\n",
        "    for i in range(len(q1)):\n",
        "        max_j = 0.0\n",
        "        for j in range(len(q2)):\n",
        "            if R[i, j] > max_j:\n",
        "                max_j = R[i, j]\n",
        "        sum_Y += max_j\n",
        "        \n",
        "    if (float(len(q1)) + float(len(q2))) == 0.0:\n",
        "        return 0.0\n",
        "        \n",
        "    overall = (sum_X + sum_Y) / (2 * (float(len(q1)) + float(len(q2))))\n",
        "\n",
        "    return overall\n",
        "\n",
        "STOP_WORDS = nltk.corpus.stopwords.words()\n",
        "def clean_sentence(val):\n",
        "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
        "    regex = re.compile('([^\\s\\w]|_)+')\n",
        "    sentence = regex.sub('', val).lower()\n",
        "    sentence = sentence.split(\" \")\n",
        "\n",
        "    for word in list(sentence):\n",
        "        if word in STOP_WORDS:\n",
        "            sentence.remove(word)\n",
        "\n",
        "    sentence = \" \".join(sentence)\n",
        "    return sentence"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw99qTFJi3I7"
      },
      "source": [
        "def semanticSimilarity(q1, q2):\n",
        "\n",
        "    tokens_q1, tokens_q2 = tokenize(q1, q2)\n",
        "    # stem_q1, stem_q2 = stemmer(tokens_q1, tokens_q2)\n",
        "    tag_q1, tag_q2 = posTag(tokens_q1, tokens_q2)\n",
        "\n",
        "    sentence = []\n",
        "    for i, word in enumerate(tag_q1):\n",
        "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
        "            sentence.append(word[0])\n",
        "\n",
        "    sense1 = Lesk(sentence)\n",
        "    sentence1Means = []\n",
        "    for word in sentence:\n",
        "        sentence1Means.append(sense1.lesk(word, sentence))\n",
        "\n",
        "    sentence = []\n",
        "    for i, word in enumerate(tag_q2):\n",
        "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
        "            sentence.append(word[0])\n",
        "\n",
        "    sense2 = Lesk(sentence)\n",
        "    sentence2Means = []\n",
        "    for word in sentence:\n",
        "        sentence2Means.append(sense2.lesk(word, sentence))\n",
        "    # for i, word in enumerate(sentence1Means):\n",
        "    #     print sentence1Means[i][0], sentence2Means[i][0]\n",
        "\n",
        "    R1 = computePath(sentence1Means, sentence2Means)\n",
        "    R2 = computeWup(sentence1Means, sentence2Means)\n",
        "\n",
        "    R = (R1 + R2) / 2\n",
        "\n",
        "    # print R\n",
        "\n",
        "    return overallSim(sentence1Means, sentence2Means, R)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91iJvdOuguG"
      },
      "source": [
        "#Node structure for graph\n",
        "class Node:\n",
        "\n",
        "    def __init__(self,src,dest,wt):\n",
        "        self.src = src\n",
        "        self.dest = dest\n",
        "        self.wt = wt\n",
        "\n",
        "\n",
        "#Class to represent an un-directed graph using adjacency list representation \n",
        "class Graph: \n",
        "   \n",
        "    def __init__(self,vertices): \n",
        "        self.V = vertices #No. of vertices \n",
        "        self.V_org = vertices \n",
        "        self.graph = defaultdict(list) # default dictionary to store graph \n",
        "        \n",
        "        \n",
        "    # function to add an edge to graph \n",
        "    def addEdge(self,u,v,w): \n",
        "        self.graph[u].append(Node(u,v,w))\n",
        "        self.graph[v].append(Node(v,u,w))\n",
        "\n",
        "        \n",
        "    #function to print graph\n",
        "    def printGraph(self):\n",
        "        s = \"\"\n",
        "        for i in self.graph:\n",
        "            s = s + str(i) + \" is connected to \"\n",
        "            print(str(i) + \" is connected to \")\n",
        "            for node in self.graph[i]:\n",
        "                s = s + str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \"\n",
        "                print(str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \")\n",
        "            s = s + \"\\n\"\n",
        "            print(\"\\n\")\n",
        "        return s\n",
        "    \n",
        "\n",
        "    #function to get BFS results for a given node till the given level\n",
        "    def BFS(self, s, max_levels):\n",
        "        visited = set()\n",
        " \n",
        "        queue = []\n",
        " \n",
        "        queue.append(s)\n",
        "        visited.add(s)\n",
        "        level = 0\n",
        "        result = {}\n",
        "        while queue:\n",
        "            aux = []\n",
        "            result[level] = []\n",
        "            \n",
        "            while queue:\n",
        "                s = queue.pop(0)\n",
        "                visited.add(s)\n",
        "                result[level].append(s)\n",
        "                for node in self.graph[s]:\n",
        "                    if node.dest not in visited:\n",
        "                        aux.append(node.dest)\n",
        "                        visited.add(node.dest)\n",
        "            level += 1\n",
        "            if level > max_levels:\n",
        "                break\n",
        "            for node in aux:\n",
        "                queue.append(node)\n",
        "            \n",
        "        return result   \n",
        "\n",
        "\n",
        "\n",
        "    def BFSi(self, s, max_levels):\n",
        "        visited = set()\n",
        " \n",
        "        queue = []\n",
        " \n",
        "        queue.append((s,0,0,1))\n",
        "        visited.add(s)\n",
        "        level = 0\n",
        "        result = {}\n",
        "        while queue:\n",
        "            aux = []\n",
        "            result[level] = []\n",
        "            \n",
        "            while queue:\n",
        "                s = queue.pop(0)\n",
        "                visited.add(s[0])\n",
        "                result[level].append(s)\n",
        "                for node in self.graph[s[0]]:\n",
        "                    if node.dest not in visited:\n",
        "                        \n",
        "#                         Wordnet Similarity\n",
        "                        q1 = clean_sentence(s[0])\n",
        "                        q2 = clean_sentence(node.dest)\n",
        "                        sim = 0\n",
        "                        sim = semanticSimilarity(q1, q2)\n",
        "\n",
        "                        sumOfCooccurence = 0\n",
        "                        for chi in self.graph[node.dest]:\n",
        "                            if chi.dest in visited:\n",
        "                                sumOfCooccurence += chi.wt\n",
        "                        aux.append((node.dest, sumOfCooccurence, level+1, sim))        \n",
        "                        visited.add(node.dest)\n",
        "            level += 1\n",
        "            if level > max_levels:\n",
        "                break\n",
        "            for node in aux:\n",
        "                queue.append(node)\n",
        "            solution = []\n",
        "            for key in result:\n",
        "                for tup in result[key]:\n",
        "                    if tup[2] != 0 and (tup[1] + np.exp(tup[3]))/np.exp(tup[2]) >= 2:\n",
        "                        solution.append( ( tup[0], (tup[1] + np.exp(tup[3]))/np.exp(tup[2]) ) )\n",
        "        return solution            \n",
        "    \n",
        "    def export_network(self, filename = \"output\"):\n",
        "        filename += \".json\"\n",
        "        obj = jsonpickle.encode(self.graph)\n",
        "        with open(filename, \"w\") as outfile: \n",
        "            json.dump(obj, outfile)\n",
        "\n",
        "    def import_network(self, filename = \"output\"):\n",
        "        filename += \".json\"\n",
        "        with open(filename) as json_file:\n",
        "            data = json.load(json_file)\n",
        "            self.graph = jsonpickle.decode(data)\n",
        "            self.V = len(self.graph)\n",
        "            self.V_org = len(self.graph)\n",
        "\n",
        "\n",
        "\n",
        "class DataHandler:\n",
        "    def __init__(self, path):\n",
        "        self.type = pathlib.Path(path).suffix\n",
        "        self.dataset_location = path\n",
        "        \n",
        "#         Use the pandas dataframe to get columns \n",
        "        if self.type == '.csv':\n",
        "#             self.df = pd.read_csv(location, nrows=100)\n",
        "            self.df = pd.read_csv(path)\n",
        "            self.df = self.df[self.df['Domain'] == \"CS \"]\n",
        "        elif self.type == '.xlsx':\n",
        "            self.df = pd.read_excel(path)\n",
        "            self.df = self.df[self.df['Domain'] == \"CS \"]\n",
        "\n",
        "    def get_dataframe(self):\n",
        "        return self.df\n",
        "\n",
        "            \n",
        "class keywordNodeNetGen:\n",
        "    \n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.subsetSize = 2\n",
        "        self.keysPair = []\n",
        "        self.keywordsList = []\n",
        "        self.dictKeywords = {}\n",
        "        self.dictKeyVsPid = {}\n",
        "        self.idList = []\n",
        "        self.colName = []\n",
        "    #     self.wbObj = openpyxl.load_workbook(self.path)\n",
        "    #     self.sheetObj = self.wbObj.active    \n",
        "\n",
        "    # #function to extract list of column headers from .xlsx\n",
        "    # def extractListCol(self):\n",
        "    #     maxCol = self.sheetObj.max_column    \n",
        "    #     # Loop will print all columns name \n",
        "    #     for i in range(1, maxCol + 1): \n",
        "    #         cellObj = self.sheetObj.cell(row = 1, column = i) \n",
        "    #         self.colName.append(cellObj.value) \n",
        "    \n",
        "\n",
        "    # #function to extract column index from its header\n",
        "    # def extractColNumber(self, strIn):\n",
        "    #     self.extractListCol()\n",
        "    #     for x in self.colName:\n",
        "    #         if x == strIn:\n",
        "    #             colIndex = self.colName.index(x)+1\n",
        "    #     return colIndex\n",
        "\n",
        "\n",
        "\n",
        "    #function to extract list of keywords in all the research papers\n",
        "    def extractDictOfKeywords(self):\n",
        "        dh = DataHandler(self.path)\n",
        "        df = dh.get_dataframe()\n",
        "        # maxRow = 1000\n",
        "        # keyIndex = self.extractColNumber('keywords')\n",
        "        rowIndices = df.index.tolist()\n",
        "        i=0\n",
        "        # for rowIndex in df.index.tolist()\n",
        "        for keyList in df['Yake keywords']:\n",
        "            if(i<2000): \n",
        "                self.dictKeywords[rowIndices[i]] = [keyword.strip().lower() for keyword in re.split(\";\", keyList) if keyword]\n",
        "                i=i+1\n",
        "\n",
        "        return self.dictKeywords\n",
        "\n",
        "\n",
        "    #function to create list of keywords in all papers\n",
        "    def generateKeywordsList(self):\n",
        "        self.dictKeywords = self.extractDictOfKeywords()\n",
        "        # print(len(self.dictKeywords))\n",
        "        self.keywordsList = list(set().union(*self.dictKeywords.values()))\n",
        "        return self.keywordsList\n",
        "    \n",
        "    \n",
        "    #function to find combinations of any 2 keywords\n",
        "    def rSubset(self): \n",
        "        self.keywordsList = self.generateKeywordsList()\n",
        "        self.keysPair = list(combinations(self.keywordsList, self.subsetSize)) \n",
        "        return self.keysPair\n",
        "    \n",
        "    \n",
        "    #function to extract weight\n",
        "    def pathWeight(self, x, y, dictKeywords, idList):\n",
        "        edgeWeight = 0\n",
        "        for idNo in idList:\n",
        "            if(x in dictKeywords[idNo] and y in dictKeywords[idNo]):\n",
        "                edgeWeight = edgeWeight + 1\n",
        "        return edgeWeight\n",
        "    \n",
        "    \n",
        "    #function to generate dictionary having keywords as keys and list of paper ids as value list \n",
        "    def generateDictKeyVsPid(self):\n",
        "        self.keywordsList = self.generateKeywordsList()\n",
        "        self.idList = self.dictKeywords.keys()\n",
        "        self.dictKeyVsPid = {key:[pid for pid in self.idList if key in self.dictKeywords[pid]] for key in self.keywordsList}\n",
        "        return self.dictKeyVsPid"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgdFdgVp147a"
      },
      "source": [
        "dh = DataHandler('drive/My Drive/Colab Notebooks/Data.xlsx')\n",
        "dataset = dh.get_dataframe()\n",
        "dataset = dataset.reset_index()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMdlxhcew_BO"
      },
      "source": [
        "# Gives sorted dictionary result of papers and number of matching keywords \n",
        "# between input keywords and rake extracted keywords from abstract\n",
        "def predict_rake(input_keywords, common_words_count):\n",
        "    res = {}\n",
        "    ran = len(dataset['Rake keywords'])\n",
        "    for i in range(1000):\n",
        "        val = dataset['Rake keywords'][i]\n",
        "        keywords = val.split(\";\")\n",
        "        com = calc_common(input_keywords, keywords)\n",
        "        if com >= common_words_count:\n",
        "            res[i] = com\n",
        "    res = dict(sorted(res.items(), key = lambda kv:(kv[1], kv[0]), reverse = True))\n",
        "    for key in res:\n",
        "        res[key] = [res[key], dataset['area'][key].strip().lower()]\n",
        "    return res\n",
        "\n",
        "# Gives sorted dictionary result of papers and number of matching keywords \n",
        "# between input keywords and yake extracted keywords from abstract\n",
        "def predict_yake(input_keywords, common_words_count):\n",
        "    res = {}\n",
        "    ran = len(dataset['Yake keywords'])\n",
        "    for i in range(1000):\n",
        "        val = dataset['Yake keywords'][i]\n",
        "        keywords = val.split(\";\")\n",
        "        com = calc_common(input_keywords, keywords)\n",
        "        if com >= common_words_count:\n",
        "            res[i] = com\n",
        "    res = dict(sorted(res.items(), key = lambda kv:(kv[1], kv[0]), reverse = True))\n",
        "    for key in res:\n",
        "        res[key] = [res[key], dataset['area'][key].strip().lower()]\n",
        "    return res\n",
        "\n",
        "# Gives sorted dictionary result of papers and number of matching keywords \n",
        "# between input keywords and author labelled keywords from abstract\n",
        "def predict_author_labelled(input_keywords, common_words_count):\n",
        "    res = {}\n",
        "    ran = len(dataset['keywords'])\n",
        "    for i in range(1000):\n",
        "        val = dataset['keywords'][i]\n",
        "        keywords = val.split(\";\")\n",
        "        com = calc_common(input_keywords, keywords)\n",
        "        if com >= common_words_count:\n",
        "            res[i] = com\n",
        "    res = dict(sorted(res.items(), key = lambda kv:(kv[1], kv[0]), reverse = True))\n",
        "    for key in res:\n",
        "        res[key] = [res[key], dataset['area'][key].strip().lower()]\n",
        "    return res\n",
        "\n",
        "def calc_common(input_keywords, keywords):\n",
        "    cou = 0\n",
        "    for w1 in input_keywords:\n",
        "        for w2 in keywords:\n",
        "            if(w1.strip().lower() == w2.strip().lower()):\n",
        "                cou+=1\n",
        "    return cou"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyU9o2LwcC43"
      },
      "source": [
        "ga1 = Graph(10000)\n",
        "ga1.import_network(\"drive/My Drive/Colab Notebooks/author/output\")\n",
        "\n",
        "ga2 = Graph(10000)\n",
        "ga2.import_network(\"drive/My Drive/Colab Notebooks/author/output2k\")\n",
        "\n",
        "gr1 = Graph(10000)\n",
        "gr1.import_network(\"drive/My Drive/Colab Notebooks/rake/output\")\n",
        "\n",
        "gr2 = Graph(10000)\n",
        "gr2.import_network(\"drive/My Drive/Colab Notebooks/rake/output2k\")\n",
        "\n",
        "gy1 = Graph(10000)\n",
        "gy1.import_network(\"drive/My Drive/Colab Notebooks/yake/output\")\n",
        "\n",
        "gy2 = Graph(10000)\n",
        "gy2.import_network(\"drive/My Drive/Colab Notebooks/yake/output2k\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAksnSlLVMKA"
      },
      "source": [
        "queryKeyAuthorList1 = ['cloud computing', 'computer']\n",
        "querySetAuthor1 = []\n",
        "for queryKeyAuthor1 in queryKeyAuthorList1:\n",
        "  queryKeyAuthor1 = queryKeyAuthor1.strip().lower()\n",
        "  queryDictAuthor1 = ga1.BFSi(queryKeyAuthor1, 3)\n",
        "  querySetAuthor1.extend([queryKeyAuthor1])\n",
        "  querySetAuthor1.extend([item[0] for item in queryDictAuthor1])\n",
        "\n",
        "queryKeyAuthorList2 = ['cloud computing', 'computer']\n",
        "querySetAuthor2 = []\n",
        "for queryKeyAuthor2 in queryKeyAuthorList2:\n",
        "  queryKeyAuthor2 = queryKeyAuthor2.strip().lower()\n",
        "  queryDictAuthor2 = ga2.BFSi(queryKeyAuthor2, 3)\n",
        "  querySetAuthor2.extend([queryKeyAuthor2])\n",
        "  querySetAuthor2.extend([item[0] for item in queryDictAuthor2])\n",
        "\n",
        "queryKeyYakeList1 = ['cloud computing', 'computer']\n",
        "querySetYake1 = []\n",
        "for queryKeyYake1 in queryKeyYakeList1:\n",
        "  queryKeyYake1 = queryKeyYake1.strip().lower()\n",
        "  queryDictYake1 = gy1.BFSi(queryKeyYake1, 3)\n",
        "  querySetYake1.extend([queryKeyYake1])\n",
        "  querySetYake1.extend([item[0] for item in queryDictYake1])\n",
        "\n",
        "queryKeyYakeList2 = ['cloud computing', 'computer']\n",
        "querySetYake2 = []\n",
        "for queryKeyYake2 in queryKeyYakeList2:\n",
        "  queryKeyYake2 = queryKeyYake2.strip().lower()\n",
        "  queryDictYake2 = gy2.BFSi(queryKeyYake2, 3)\n",
        "  querySetYake2.extend([queryKeyYake2])\n",
        "  querySetYake2.extend([item[0] for item in queryDictYake2])\n",
        "\n",
        "queryKeyRakeList1 = ['cloud computing', 'computer']\n",
        "querySetRake1 = []\n",
        "for queryKeyRake1 in queryKeyRakeList1:\n",
        "  queryKeyRake1 = queryKeyRake1.strip().lower()\n",
        "  queryDictRake1 = gr1.BFSi(queryKeyRake1, 3)\n",
        "  querySetRake1.extend([queryKeyRake1])\n",
        "  querySetRake1.extend([item[0] for item in queryDictRake1])\n",
        "\n",
        "queryKeyRakeList2 = ['cloud computing', 'computer']\n",
        "querySetRake2 = []\n",
        "for queryKeyRake2 in queryKeyRakeList2:\n",
        "  queryKeyRake2 = queryKeyRake2.strip().lower()\n",
        "  queryDictRake2 = gr2.BFSi(queryKeyRake2, 3)\n",
        "  querySetRake2.extend([queryKeyRake2])\n",
        "  querySetRake2.extend([item[0] for item in queryDictRake2])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq-Kgr8NWUAl",
        "outputId": "ebbfd50a-30e3-43ce-e972-1baad33ccc99"
      },
      "source": [
        "print(querySetAuthor1)\n",
        "\n",
        "print(querySetAuthor2)\n",
        "\n",
        "print(querySetYake1)\n",
        "\n",
        "print(querySetYake2)\n",
        "\n",
        "print(querySetRake1)\n",
        "\n",
        "print(querySetRake2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cloud computing', 'security', 'privacy', 'computer']\n",
            "['cloud computing', 'hadoop', 'distributed computing', 'security', 'privacy', 'big data', 'optimization', 'computer']\n",
            "['cloud computing', 'cloud', 'information technology', 'computing', 'computer', 'graphics']\n",
            "['cloud computing', 'cloud', 'distributed computing', 'information technology', 'computing', 'computer', 'programming', 'graphics', 'computer programming', 'computer science']\n",
            "['cloud computing', 'cloud', 'computing', 'computer', 'computer graphics']\n",
            "['cloud computing', 'cloud', 'information technology', 'distributed computing', 'computing', 'computer', 'computer programming', 'computer science', 'graphics', 'programming']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jegO192uWhfF"
      },
      "source": [
        "resultAuthor1 = predict_author_labelled(querySetAuthor1, 3)\n",
        "\n",
        "resultAuthor2 = predict_author_labelled(querySetAuthor2, 3)\n",
        "\n",
        "resultYake1 = predict_yake(querySetYake1, 3)\n",
        "\n",
        "resultYake2 = predict_yake(querySetYake2, 3)\n",
        "\n",
        "resultRake1 = predict_rake(querySetRake1, 3)\n",
        "\n",
        "resultRake2 = predict_rake(querySetRake2, 3)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etb_DMYxXLXx",
        "outputId": "1887bb63-aca2-4271-cca4-531da7c755f6"
      },
      "source": [
        "print(resultAuthor1)\n",
        "\n",
        "print(resultAuthor2)\n",
        "\n",
        "print(resultYake1)\n",
        "\n",
        "print(resultYake2)\n",
        "\n",
        "print(resultRake1)\n",
        "\n",
        "print(resultRake2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{902: [3, 'distributed computing'], 326: [3, 'image processing']}\n",
            "{902: [3, 'distributed computing'], 786: [3, 'distributed computing'], 326: [3, 'image processing']}\n",
            "{911: [4, 'network security'], 902: [4, 'distributed computing'], 912: [3, 'distributed computing'], 909: [3, 'distributed computing'], 907: [3, 'distributed computing'], 905: [3, 'network security'], 895: [3, 'distributed computing']}\n",
            "{911: [4, 'network security'], 902: [4, 'distributed computing'], 912: [3, 'distributed computing'], 909: [3, 'distributed computing'], 907: [3, 'distributed computing'], 905: [3, 'network security'], 900: [3, 'distributed computing'], 895: [3, 'distributed computing']}\n",
            "{912: [3, 'distributed computing'], 911: [3, 'network security'], 909: [3, 'distributed computing'], 905: [3, 'network security'], 902: [3, 'distributed computing'], 895: [3, 'distributed computing']}\n",
            "{911: [4, 'network security'], 902: [4, 'distributed computing'], 912: [3, 'distributed computing'], 909: [3, 'distributed computing'], 907: [3, 'distributed computing'], 905: [3, 'network security'], 900: [3, 'distributed computing'], 895: [3, 'distributed computing']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWLPur7sYrSG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}