{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordnet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP7B8lO0RifR"
      },
      "source": [
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import math\r\n",
        "import nltk\r\n",
        "from nltk.corpus import wordnet as wn\r\n",
        "from nltk import word_tokenize\r\n",
        "from scipy import spatial\r\n",
        "from nltk.metrics import edit_distance\r\n",
        "\r\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYLOhTxpVbsr",
        "outputId": "1c40f91c-dc63-4c47-8a76-ff18c6cf82c8"
      },
      "source": [
        "from google.colab import drive\r\n",
        "# drive.mount('/content/gdrive')\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHIUthndTC8N",
        "outputId": "6452f54c-d6b9-45b3-d336-8e7ff61b8b6c"
      },
      "source": [
        "nltk.download('stopwords')  \r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbqOGNvpR598"
      },
      "source": [
        "def tokenize(q1, q2):\r\n",
        "    \"\"\"\r\n",
        "        q1 and q2 are sentences/questions. Function returns a list of tokens for both.\r\n",
        "    \"\"\"\r\n",
        "    return word_tokenize(q1), word_tokenize(q2)\r\n",
        "\r\n",
        "\r\n",
        "def posTag(q1, q2):\r\n",
        "    \"\"\"\r\n",
        "        q1 and q2 are lists. Function returns a list of POS tagged tokens for both.\r\n",
        "    \"\"\"\r\n",
        "    return nltk.pos_tag(q1), nltk.pos_tag(q2)\r\n",
        "\r\n",
        "\r\n",
        "def stemmer(tag_q1, tag_q2):\r\n",
        "    \"\"\"\r\n",
        "        tag_q = tagged lists. Function returns a stemmed list.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    stem_q1 = []\r\n",
        "    stem_q2 = []\r\n",
        "\r\n",
        "    for token in tag_q1:\r\n",
        "        stem_q1.append(stem(token))\r\n",
        "\r\n",
        "    for token in tag_q2:\r\n",
        "        stem_q2.append(stem(token))\r\n",
        "\r\n",
        "    return stem_q1, stem_q2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG-78BxjSEVx"
      },
      "source": [
        "class Lesk(object):\r\n",
        "\r\n",
        "    def __init__(self, sentence):\r\n",
        "        self.sentence = sentence\r\n",
        "        self.meanings = {}\r\n",
        "        for word in sentence:\r\n",
        "            self.meanings[word] = ''\r\n",
        "\r\n",
        "    def getSenses(self, word):\r\n",
        "        # print word\r\n",
        "        return wn.synsets(word.lower())\r\n",
        "\r\n",
        "    def getGloss(self, senses):\r\n",
        "\r\n",
        "        gloss = {}\r\n",
        "\r\n",
        "        for sense in senses:\r\n",
        "            gloss[sense.name()] = []\r\n",
        "\r\n",
        "        for sense in senses:\r\n",
        "            gloss[sense.name()] += word_tokenize(sense.definition())\r\n",
        "\r\n",
        "        return gloss\r\n",
        "\r\n",
        "    def getAll(self, word):\r\n",
        "        senses = self.getSenses(word)\r\n",
        "\r\n",
        "        if senses == []:\r\n",
        "            return {word.lower(): senses}\r\n",
        "\r\n",
        "        return self.getGloss(senses)\r\n",
        "\r\n",
        "    def Score(self, set1, set2):\r\n",
        "        # Base\r\n",
        "        overlap = 0\r\n",
        "\r\n",
        "        # Step\r\n",
        "        for word in set1:\r\n",
        "            if word in set2:\r\n",
        "                overlap += 1\r\n",
        "\r\n",
        "        return overlap\r\n",
        "\r\n",
        "    def overlapScore(self, word1, word2):\r\n",
        "\r\n",
        "        gloss_set1 = self.getAll(word1)\r\n",
        "        if self.meanings[word2] == '':\r\n",
        "            gloss_set2 = self.getAll(word2)\r\n",
        "        else:\r\n",
        "            # print 'here'\r\n",
        "            gloss_set2 = self.getGloss([wn.synset(self.meanings[word2])])\r\n",
        "\r\n",
        "        # print gloss_set2\r\n",
        "\r\n",
        "        score = {}\r\n",
        "        for i in gloss_set1.keys():\r\n",
        "            score[i] = 0\r\n",
        "            for j in gloss_set2.keys():\r\n",
        "                score[i] += self.Score(gloss_set1[i], gloss_set2[j])\r\n",
        "\r\n",
        "        bestSense = None\r\n",
        "        max_score = 0\r\n",
        "        for i in gloss_set1.keys():\r\n",
        "            if score[i] > max_score:\r\n",
        "                max_score = score[i]\r\n",
        "                bestSense = i\r\n",
        "\r\n",
        "        return bestSense, max_score\r\n",
        "\r\n",
        "    def lesk(self, word, sentence):\r\n",
        "        maxOverlap = 0\r\n",
        "        context = sentence\r\n",
        "        word_sense = []\r\n",
        "        meaning = {}\r\n",
        "\r\n",
        "        senses = self.getSenses(word)\r\n",
        "\r\n",
        "        for sense in senses:\r\n",
        "            meaning[sense.name()] = 0\r\n",
        "\r\n",
        "        for word_context in context:\r\n",
        "            if not word == word_context:\r\n",
        "                score = self.overlapScore(word, word_context)\r\n",
        "                if score[0] == None:\r\n",
        "                    continue\r\n",
        "                meaning[score[0]] += score[1]\r\n",
        "\r\n",
        "        if senses == []:\r\n",
        "            return word, None, None\r\n",
        "\r\n",
        "        self.meanings[word] = max(meaning.keys(), key=lambda x: meaning[x])\r\n",
        "\r\n",
        "        return word, self.meanings[word], wn.synset(self.meanings[word]).definition()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFBUzuRnSSf-"
      },
      "source": [
        "def path(set1, set2):\r\n",
        "    return wn.path_similarity(set1, set2)\r\n",
        "\r\n",
        "\r\n",
        "def wup(set1, set2):\r\n",
        "    return wn.wup_similarity(set1, set2)\r\n",
        "\r\n",
        "\r\n",
        "def edit(word1, word2):\r\n",
        "    if float(edit_distance(word1, word2)) == 0.0:\r\n",
        "        return 0.0\r\n",
        "    return 1.0 / float(edit_distance(word1, word2))\r\n",
        "\r\n",
        "def computePath(q1, q2):\r\n",
        "\r\n",
        "    R = np.zeros((len(q1), len(q2)))\r\n",
        "\r\n",
        "    for i in range(len(q1)):\r\n",
        "        for j in range(len(q2)):\r\n",
        "            if q1[i][1] == None or q2[j][1] == None:\r\n",
        "                sim = edit(q1[i][0], q2[j][0])\r\n",
        "            else:\r\n",
        "                sim = path(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\r\n",
        "\r\n",
        "            if sim == None:\r\n",
        "                sim = edit(q1[i][0], q2[j][0])\r\n",
        "\r\n",
        "            R[i, j] = sim\r\n",
        "\r\n",
        "    # print R\r\n",
        "\r\n",
        "    return R\r\n",
        "\r\n",
        "def computeWup(q1, q2):\r\n",
        "\r\n",
        "    R = np.zeros((len(q1), len(q2)))\r\n",
        "\r\n",
        "    for i in range(len(q1)):\r\n",
        "        for j in range(len(q2)):\r\n",
        "            if q1[i][1] == None or q2[j][1] == None:\r\n",
        "                sim = edit(q1[i][0], q2[j][0])\r\n",
        "            else:\r\n",
        "                sim = wup(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\r\n",
        "\r\n",
        "            if sim == None:\r\n",
        "                sim = edit(q1[i][0], q2[j][0])\r\n",
        "\r\n",
        "            R[i, j] = sim\r\n",
        "\r\n",
        "    # print R\r\n",
        "\r\n",
        "    return R\r\n",
        "\r\n",
        "def overallSim(q1, q2, R):\r\n",
        "\r\n",
        "    sum_X = 0.0\r\n",
        "    sum_Y = 0.0\r\n",
        "\r\n",
        "    for i in range(len(q1)):\r\n",
        "        max_i = 0.0\r\n",
        "        for j in range(len(q2)):\r\n",
        "            if R[i, j] > max_i:\r\n",
        "                max_i = R[i, j]\r\n",
        "        sum_X += max_i\r\n",
        "\r\n",
        "    for i in range(len(q1)):\r\n",
        "        max_j = 0.0\r\n",
        "        for j in range(len(q2)):\r\n",
        "            if R[i, j] > max_j:\r\n",
        "                max_j = R[i, j]\r\n",
        "        sum_Y += max_j\r\n",
        "        \r\n",
        "    if (float(len(q1)) + float(len(q2))) == 0.0:\r\n",
        "        return 0.0\r\n",
        "        \r\n",
        "    overall = (sum_X + sum_Y) / (2 * (float(len(q1)) + float(len(q2))))\r\n",
        "\r\n",
        "    return overall\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjqmUsuJSy7f"
      },
      "source": [
        "def semanticSimilarity(q1, q2):\r\n",
        "\r\n",
        "    tokens_q1, tokens_q2 = tokenize(q1, q2)\r\n",
        "    # stem_q1, stem_q2 = stemmer(tokens_q1, tokens_q2)\r\n",
        "    tag_q1, tag_q2 = posTag(tokens_q1, tokens_q2)\r\n",
        "\r\n",
        "    sentence = []\r\n",
        "    for i, word in enumerate(tag_q1):\r\n",
        "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\r\n",
        "            sentence.append(word[0])\r\n",
        "\r\n",
        "    sense1 = Lesk(sentence)\r\n",
        "    sentence1Means = []\r\n",
        "    for word in sentence:\r\n",
        "        sentence1Means.append(sense1.lesk(word, sentence))\r\n",
        "\r\n",
        "    sentence = []\r\n",
        "    for i, word in enumerate(tag_q2):\r\n",
        "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\r\n",
        "            sentence.append(word[0])\r\n",
        "\r\n",
        "    sense2 = Lesk(sentence)\r\n",
        "    sentence2Means = []\r\n",
        "    for word in sentence:\r\n",
        "        sentence2Means.append(sense2.lesk(word, sentence))\r\n",
        "    # for i, word in enumerate(sentence1Means):\r\n",
        "    #     print sentence1Means[i][0], sentence2Means[i][0]\r\n",
        "\r\n",
        "    R1 = computePath(sentence1Means, sentence2Means)\r\n",
        "    R2 = computeWup(sentence1Means, sentence2Means)\r\n",
        "\r\n",
        "    R = (R1 + R2) / 2\r\n",
        "\r\n",
        "    # print R\r\n",
        "\r\n",
        "    return overallSim(sentence1Means, sentence2Means, R)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OlwR1yxSzn2"
      },
      "source": [
        "STOP_WORDS = nltk.corpus.stopwords.words()\r\n",
        "def clean_sentence(val):\r\n",
        "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\r\n",
        "    regex = re.compile('([^\\s\\w]|_)+')\r\n",
        "    sentence = regex.sub('', val).lower()\r\n",
        "    sentence = sentence.split(\" \")\r\n",
        "\r\n",
        "    for word in list(sentence):\r\n",
        "        if word in STOP_WORDS:\r\n",
        "            sentence.remove(word)\r\n",
        "\r\n",
        "    sentence = \" \".join(sentence)\r\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZvkOxIqWDcB"
      },
      "source": [
        "path = \"drive/My Drive/Colab Notebooks/semantic/train_small.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy1TsuCTS2Ga"
      },
      "source": [
        "from sklearn.metrics import log_loss\r\n",
        "\r\n",
        "\r\n",
        "X_train = pd.read_csv(path)\r\n",
        "X_train = X_train.dropna(how=\"any\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NQwexlfV6bP",
        "outputId": "c21dadd6-2a99-46bd-9a3c-f6ca411ff902"
      },
      "source": [
        "y = X_train['is_duplicate']\r\n",
        "\r\n",
        "print('Exported Cleaned train Data, no need for cleaning')\r\n",
        "for col in ['question1', 'question2']:\r\n",
        "    X_train[col] = X_train[col].apply(clean_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exported Cleaned train Data, no need for cleaning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C7rOKH7Z0Wb"
      },
      "source": [
        "# y_pred = []\r\n",
        "# count = 0\r\n",
        "# print('Calculating similarity for the training data, please wait.')\r\n",
        "# for row in X_train.itertuples():\r\n",
        "#     # print row\r\n",
        "#     q1 = str(row[4])\r\n",
        "#     q2 = str(row[5])\r\n",
        "\r\n",
        "#     sim = semanticSimilarity(q1, q2)\r\n",
        "#     count += 1\r\n",
        "#     if count % 10000 == 0:\r\n",
        "#         print(str(count)+\", \"+str(sim)+\", \"+str(row[6]))\r\n",
        "#     y_pred.append(sim)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbhV5GOJWLlZ"
      },
      "source": [
        "# output = pd.DataFrame(list(zip(X_train['id'].tolist(), y_pred)), columns=['id', 'similarity'])\r\n",
        "# output.to_csv('drive/My Drive/Colab Notebooks/semantic/semantic_train_small.csv', index=False)\r\n",
        "\r\n",
        "# print(\"Log Loss Score:\")\r\n",
        "# print(log_loss(y, np.array(y_pred)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao12fwfon6Lc",
        "outputId": "3162ce66-c16c-4733-896c-155914df1b0e"
      },
      "source": [
        "print('Exported Cleaned train Data, no need for cleaning')\r\n",
        "q1 = \"machine learning\"   \r\n",
        "q2 = \"deep learning\"\r\n",
        "q1 = clean_sentence(q1)\r\n",
        "q2 = clean_sentence(q2)\r\n",
        "\r\n",
        "print('Calculating similarity for the training data, please wait.')\r\n",
        "\r\n",
        "\r\n",
        "sim = semanticSimilarity(q1, q2)\r\n",
        "print(sim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exported Cleaned train Data, no need for cleaning\n",
            "Calculating similarity for the training data, please wait.\n",
            "0.08571428571428572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RRROTLqoVZY"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAK3aO4GobWU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYQC6CP_tKjF"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLzjj7lTtLt_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}