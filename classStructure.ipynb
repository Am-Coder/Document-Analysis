{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYbtEG-INE1y",
        "outputId": "e89d1ca1-8a50-4ce5-bed2-d79b1455c12d"
      },
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgSsaAiOMbAP",
        "outputId": "d120b5c8-b6d4-4bd2-fffc-dc336581293e"
      },
      "source": [
        "!pip install jsonpickle"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEiIKqLR9L3E",
        "outputId": "d84b6ddd-f577-40f2-bac9-488ddd00b16a"
      },
      "source": [
        "!pip install rake_nltk"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rake_nltk in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rake_nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->rake_nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7oGf1zrMhuR",
        "outputId": "3f34ee49-16f1-4123-9389-aeb442cefd1e"
      },
      "source": [
        "nltk.download('stopwords')  \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpLlCERGyRFz"
      },
      "source": [
        "#Node structure for graph\n",
        "class Node:\n",
        "\n",
        "    def __init__(self,src,dest,wt):\n",
        "        self.src = src\n",
        "        self.dest = dest\n",
        "        self.wt = wt"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T4SZl5iyXYe"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import jsonpickle \n",
        "import math\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import numpy as np # linear algebra\n",
        "from itertools import combinations \n",
        "from collections import defaultdict \n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize\n",
        "from scipy import spatial\n",
        "from nltk.metrics import edit_distance\n",
        "from collections import defaultdict \n",
        "# from WordNet import WordNet\n",
        "# from Node import Node\n",
        "\n",
        "#Class to represent an un-directed graph using adjacency list representation \n",
        "class Graph: \n",
        "   \n",
        "    def __init__(self,vertices): \n",
        "        self.V = vertices #No. of vertices \n",
        "        self.V_org = vertices \n",
        "        self.graph = defaultdict(list) # default dictionary to store graph \n",
        "        \n",
        "        \n",
        "    # function to add an edge to graph \n",
        "    def addEdge(self,u,v,w): \n",
        "        self.graph[u].append(Node(u,v,w))\n",
        "        self.graph[v].append(Node(v,u,w))\n",
        "\n",
        "        \n",
        "    #function to print graph\n",
        "    def printGraph(self):\n",
        "        s = \"\"\n",
        "        for i in self.graph:\n",
        "            s = s + str(i) + \" is connected to \"\n",
        "            print(str(i) + \" is connected to \")\n",
        "            for node in self.graph[i]:\n",
        "                s = s + str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \"\n",
        "                print(str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \")\n",
        "            s = s + \"\\n\"\n",
        "            print(\"\\n\")\n",
        "        return s\n",
        "\n",
        "    def BFS(self, s, max_levels):\n",
        "        visited = set()\n",
        "         \n",
        "        queue = []\n",
        "        wordNet = WordNet()\n",
        "        queue.append((s,0,0,1))\n",
        "        visited.add(s)\n",
        "        level = 0\n",
        "        result = {}\n",
        "        while queue:\n",
        "            aux = []\n",
        "            result[level] = []\n",
        "            \n",
        "            while queue:\n",
        "                s = queue.pop(0)\n",
        "                visited.add(s[0])\n",
        "                result[level].append(s)\n",
        "                for node in self.graph[s[0]]:\n",
        "                    if node.dest not in visited:\n",
        "                        \n",
        "#                         Wordnet Similarity\n",
        "                        q1 = wordNet.clean_sentence(s[0])\n",
        "                        q2 = wordNet.clean_sentence(node.dest)\n",
        "                        sim = 0\n",
        "                        sim = wordNet.semanticSimilarity(q1, q2)\n",
        "\n",
        "                        sumOfCooccurence = 0\n",
        "                        for chi in self.graph[node.dest]:\n",
        "                            if chi.dest in visited:\n",
        "                                sumOfCooccurence += chi.wt\n",
        "                        aux.append((node.dest, sumOfCooccurence, level+1, sim))        \n",
        "                        visited.add(node.dest)\n",
        "            level += 1\n",
        "            if level > max_levels:\n",
        "                break\n",
        "            for node in aux:\n",
        "                queue.append(node)\n",
        "            solution = []\n",
        "            for key in result:\n",
        "                for tup in result[key]:\n",
        "                    if tup[2] != 0 and (tup[1] + np.exp(tup[3]))/np.exp(tup[2]) >= 2:\n",
        "                        solution.append( ( tup[0], (tup[1] + np.exp(tup[3]))/np.exp(tup[2]) ) )\n",
        "        return solution            \n",
        "    \n",
        "    def exportNetwork(self, filename = \"output\"):\n",
        "        filename += \".json\"\n",
        "        obj = jsonpickle.encode(self.graph)\n",
        "        with open(filename, \"w\") as outfile: \n",
        "            json.dump(obj, outfile)\n",
        "\n",
        "    def importNetwork(self, filename = \"output\"):\n",
        "        filename += \".json\"\n",
        "        with open(filename) as json_file:\n",
        "            data = json.load(json_file)\n",
        "            self.graph = jsonpickle.decode(data)\n",
        "            self.V = len(self.graph)\n",
        "            self.V_org = len(self.graph)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N78rp3h5y3CJ"
      },
      "source": [
        "import pandas as pd\n",
        "import pathlib\n",
        "\n",
        "class DataHandler:\n",
        "    def __init__(self, path):\n",
        "        self.type = pathlib.Path(path).suffix\n",
        "        self.dataset_location = path\n",
        "        \n",
        "        #Use the pandas dataframe to get columns \n",
        "        if self.type == '.csv':\n",
        "            self.df = pd.read_csv(path)\n",
        "            self.df = self.df[self.df['Domain'] == \"CS \"]\n",
        "        elif self.type == '.xlsx':\n",
        "            self.df = pd.read_excel(path)\n",
        "            self.df = self.df[self.df['Domain'] == \"CS \"]\n",
        "\n",
        "    def getDataframe(self):\n",
        "        return self.df"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTydGKAYy6f9"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import jsonpickle \n",
        "import math\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import numpy as np # linear algebra\n",
        "from itertools import combinations \n",
        "from collections import defaultdict \n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize\n",
        "from scipy import spatial\n",
        "from nltk.metrics import edit_distance\n",
        "from collections import defaultdict \n",
        "# from DataHandler import DataHandler\n",
        "# from Graph import Graph\n",
        "\n",
        "class PredictPaper:\n",
        "    def __init__(self, dataPath, graphPath):\n",
        "        self.dh = DataHandler(dataPath)\n",
        "        self.dataset = self.dh.getDataframe()\n",
        "        self.dataset = self.dataset.reset_index()\n",
        "        self.g = Graph(10000)\n",
        "        self.g.importNetwork(graphPath)\n",
        "        self.querySet = []\n",
        "        self.result = {}\n",
        "\n",
        "\n",
        "    def predictUtil(self, queryKeyList):\n",
        "        for queryKey in queryKeyList:\n",
        "            queryKey = queryKey.strip().lower()\n",
        "            queryDict = self.g.BFS(queryKey, 3)\n",
        "            self.querySet.extend([queryKey])\n",
        "            self.querySet.extend([item[0] for item in queryDict])\n",
        "        return self.querySet\n",
        "\n",
        "\n",
        "    # Gives sorted dictionary result of papers and number of matching keywords \n",
        "    # between input keywords and rake extracted keywords from abstract\n",
        "    def predict(self, queryKeyList, commonWordsCount):\n",
        "        self.querySet = self.predictUtil(queryKeyList)\n",
        "        ran = len(self.dataset['Rake keywords'])\n",
        "        for i in range(1000):\n",
        "            val = self.dataset['Rake keywords'][i]\n",
        "            keywords = val.split(\";\")\n",
        "            com = self.calcCommon(keywords)\n",
        "            if com >= commonWordsCount:\n",
        "                self.result[i] = com\n",
        "        self.result = dict(sorted(self.result.items(), key = lambda kv:(kv[1], kv[0]), reverse = True))\n",
        "        for key in self.result:\n",
        "            self.result[key] = [self.result[key], self.dataset['area'][key].strip().lower()]\n",
        "        return self.result\n",
        "\n",
        "    def calcCommon(self, keywords):\n",
        "        cou = 0\n",
        "        for w1 in self.querySet:\n",
        "            for w2 in keywords:\n",
        "                if(w1.strip().lower() == w2.strip().lower()):\n",
        "                    cou+=1\n",
        "        return cou"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydf8fhpm77E9"
      },
      "source": [
        "from rake_nltk import Metric, Rake\n",
        "\n",
        "class KeywordExtractor:\n",
        "    def __init__(self):\n",
        "        self.extractedKeywords = []\n",
        "\n",
        "    def rakealgo(self, abstract):\n",
        "        r = Rake(min_length=1, max_length=2)\n",
        "        r.extract_keywords_from_text(abstract)\n",
        "        result = r.get_ranked_phrases()\n",
        "        topResult = result[:10]\n",
        "        self.extractedKeywords.append(topResult)\n",
        "        return self.extractedKeywords"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhgF9w1A9XdM"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import jsonpickle \n",
        "import math\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import numpy as np # linear algebra\n",
        "from itertools import combinations \n",
        "from collections import defaultdict \n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize\n",
        "from scipy import spatial\n",
        "from nltk.metrics import edit_distance\n",
        "from collections import defaultdict \n",
        "\n",
        "\n",
        "class Lesk(object):\n",
        "\n",
        "    def __init__(self, sentence):\n",
        "        self.sentence = sentence\n",
        "        self.meanings = {}\n",
        "        for word in sentence:\n",
        "            self.meanings[word] = ''\n",
        "\n",
        "    def getSenses(self, word):\n",
        "        # print word\n",
        "        return wn.synsets(word.lower())\n",
        "\n",
        "    def getGloss(self, senses):\n",
        "\n",
        "        gloss = {}\n",
        "\n",
        "        for sense in senses:\n",
        "            gloss[sense.name()] = []\n",
        "\n",
        "        for sense in senses:\n",
        "            gloss[sense.name()] += word_tokenize(sense.definition())\n",
        "\n",
        "        return gloss\n",
        "\n",
        "    def getAll(self, word):\n",
        "        senses = self.getSenses(word)\n",
        "\n",
        "        if senses == []:\n",
        "            return {word.lower(): senses}\n",
        "\n",
        "        return self.getGloss(senses)\n",
        "\n",
        "    def Score(self, set1, set2):\n",
        "        # Base\n",
        "        overlap = 0\n",
        "\n",
        "        # Step\n",
        "        for word in set1:\n",
        "            if word in set2:\n",
        "                overlap += 1\n",
        "\n",
        "        return overlap\n",
        "\n",
        "    def overlapScore(self, word1, word2):\n",
        "\n",
        "        gloss_set1 = self.getAll(word1)\n",
        "        if self.meanings[word2] == '':\n",
        "            gloss_set2 = self.getAll(word2)\n",
        "        else:\n",
        "            # print 'here'\n",
        "            gloss_set2 = self.getGloss([wn.synset(self.meanings[word2])])\n",
        "\n",
        "        # print gloss_set2\n",
        "\n",
        "        score = {}\n",
        "        for i in gloss_set1.keys():\n",
        "            score[i] = 0\n",
        "            for j in gloss_set2.keys():\n",
        "                score[i] += self.Score(gloss_set1[i], gloss_set2[j])\n",
        "\n",
        "        bestSense = None\n",
        "        max_score = 0\n",
        "        for i in gloss_set1.keys():\n",
        "            if score[i] > max_score:\n",
        "                max_score = score[i]\n",
        "                bestSense = i\n",
        "\n",
        "        return bestSense, max_score\n",
        "\n",
        "    def lesk(self, word, sentence):\n",
        "        maxOverlap = 0\n",
        "        context = sentence\n",
        "        word_sense = []\n",
        "        meaning = {}\n",
        "\n",
        "        senses = self.getSenses(word)\n",
        "\n",
        "        for sense in senses:\n",
        "            meaning[sense.name()] = 0\n",
        "\n",
        "        for word_context in context:\n",
        "            if not word == word_context:\n",
        "                score = self.overlapScore(word, word_context)\n",
        "                if score[0] == None:\n",
        "                    continue\n",
        "                meaning[score[0]] += score[1]\n",
        "\n",
        "        if senses == []:\n",
        "            return word, None, None\n",
        "\n",
        "        self.meanings[word] = max(meaning.keys(), key=lambda x: meaning[x])\n",
        "\n",
        "        return word, self.meanings[word], wn.synset(self.meanings[word]).definition()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpqEBXx2MOOz"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import numpy as np # linear algebra\n",
        "from itertools import combinations \n",
        "from collections import defaultdict \n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize\n",
        "from scipy import spatial\n",
        "from nltk.metrics import edit_distance\n",
        "from collections import defaultdict \n",
        "# from Lesk import Lesk\n",
        "\n",
        "class WordNet:\n",
        "     \n",
        "    def __init__(self): \n",
        "        self.STOP_WORDS = nltk.corpus.stopwords.words()\n",
        "        \n",
        "    def tokenize(self, q1, q2):\n",
        "        return word_tokenize(q1), word_tokenize(q2)\n",
        "\n",
        "\n",
        "    def posTag(self, q1, q2):\n",
        "        return nltk.pos_tag(q1), nltk.pos_tag(q2)\n",
        "\n",
        "\n",
        "    def stemmer(self, tag_q1, tag_q2):\n",
        "        stem_q1 = []\n",
        "        stem_q2 = []\n",
        "\n",
        "        for token in tag_q1:\n",
        "            stem_q1.append(stem(token))\n",
        "\n",
        "        for token in tag_q2:\n",
        "            stem_q2.append(stem(token))\n",
        "\n",
        "        return stem_q1, stem_q2\n",
        "    \n",
        "    def path(self, set1, set2):\n",
        "        return wn.path_similarity(set1, set2)\n",
        "\n",
        "\n",
        "    def wup(self, set1, set2):\n",
        "        return wn.wup_similarity(set1, set2)\n",
        "\n",
        "\n",
        "    def edit(self, word1, word2):\n",
        "        if float(edit_distance(word1, word2)) == 0.0:\n",
        "            return 0.0\n",
        "        return 1.0 / float(edit_distance(word1, word2))\n",
        "\n",
        "    def computePath(self, q1, q2):\n",
        "\n",
        "        R = np.zeros((len(q1), len(q2)))\n",
        "\n",
        "        for i in range(len(q1)):\n",
        "            for j in range(len(q2)):\n",
        "                if q1[i][1] == None or q2[j][1] == None:\n",
        "                    sim = self.edit(q1[i][0], q2[j][0])\n",
        "                else:\n",
        "                    sim = self.path(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
        "\n",
        "                if sim == None:\n",
        "                    sim = self.edit(q1[i][0], q2[j][0])\n",
        "\n",
        "                R[i, j] = sim\n",
        "\n",
        "        # print R\n",
        "\n",
        "        return R\n",
        "\n",
        "    def computeWup(self, q1, q2):\n",
        "        \n",
        "        R = np.zeros((len(q1), len(q2)))\n",
        "\n",
        "        for i in range(len(q1)):\n",
        "            for j in range(len(q2)):\n",
        "                if q1[i][1] == None or q2[j][1] == None:\n",
        "                    sim = self.edit(q1[i][0], q2[j][0])\n",
        "                else:\n",
        "                    sim = self.wup(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
        "\n",
        "                if sim == None:\n",
        "                    sim = self.edit(q1[i][0], q2[j][0])\n",
        "\n",
        "                R[i, j] = sim\n",
        "\n",
        "        # print R\n",
        "\n",
        "        return R\n",
        "\n",
        "    def overallSim(self, q1, q2, R):\n",
        "\n",
        "        sum_X = 0.0\n",
        "        sum_Y = 0.0\n",
        "\n",
        "        for i in range(len(q1)):\n",
        "            max_i = 0.0\n",
        "            for j in range(len(q2)):\n",
        "                if R[i, j] > max_i:\n",
        "                    max_i = R[i, j]\n",
        "            sum_X += max_i\n",
        "\n",
        "        for i in range(len(q1)):\n",
        "            max_j = 0.0\n",
        "            for j in range(len(q2)):\n",
        "                if R[i, j] > max_j:\n",
        "                    max_j = R[i, j]\n",
        "            sum_Y += max_j\n",
        "\n",
        "        if (float(len(q1)) + float(len(q2))) == 0.0:\n",
        "            return 0.0\n",
        "\n",
        "        overall = (sum_X + sum_Y) / (2 * (float(len(q1)) + float(len(q2))))\n",
        "\n",
        "        return overall\n",
        "\n",
        "    def clean_sentence(self, val):\n",
        "        \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
        "        regex = re.compile('([^\\s\\w]|_)+')\n",
        "        sentence = regex.sub('', val).lower()\n",
        "        sentence = sentence.split(\" \")\n",
        "\n",
        "        for word in list(sentence):\n",
        "            if word in self.STOP_WORDS:\n",
        "                sentence.remove(word)\n",
        "\n",
        "        sentence = \" \".join(sentence)\n",
        "        return sentence\n",
        "    \n",
        "    def semanticSimilarity(self, q1, q2):\n",
        "        tokens_q1, tokens_q2 = self.tokenize(q1, q2)\n",
        "        # stem_q1, stem_q2 = stemmer(tokens_q1, tokens_q2)\n",
        "        tag_q1, tag_q2 = self.posTag(tokens_q1, tokens_q2)\n",
        "\n",
        "        sentence = []\n",
        "        for i, word in enumerate(tag_q1):\n",
        "            if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
        "                sentence.append(word[0])\n",
        "\n",
        "        sense1 = Lesk(sentence)\n",
        "        sentence1Means = []\n",
        "        for word in sentence:\n",
        "            sentence1Means.append(sense1.lesk(word, sentence))\n",
        "\n",
        "        sentence = []\n",
        "        for i, word in enumerate(tag_q2):\n",
        "            if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
        "                sentence.append(word[0])\n",
        "\n",
        "        sense2 = Lesk(sentence)\n",
        "        sentence2Means = []\n",
        "        for word in sentence:\n",
        "            sentence2Means.append(sense2.lesk(word, sentence))\n",
        "        # for i, word in enumerate(sentence1Means):\n",
        "        #     print sentence1Means[i][0], sentence2Means[i][0]\n",
        "\n",
        "        R1 = self.computePath(sentence1Means, sentence2Means)\n",
        "        R2 = self.computeWup(sentence1Means, sentence2Means)\n",
        "\n",
        "        R = (R1 + R2) / 2\n",
        "\n",
        "        # print R\n",
        "\n",
        "        return self.overallSim(sentence1Means, sentence2Means, R)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmcfoBTPOn6F",
        "outputId": "85ac913d-6b56-4446-bcc1-c18820b3d5cb"
      },
      "source": [
        "# from PredictPaper import PredictPaper\n",
        "\n",
        "# Driver Function \n",
        "if __name__ == \"__main__\": \n",
        "    dataPath = \"drive/My Drive/Colab Notebooks/Data.xlsx\"\n",
        "    graphPath = \"drive/My Drive/Colab Notebooks/rake/output2k\"\n",
        "    predPap = PredictPaper(dataPath, graphPath)\n",
        "    inputKey  = ['cloud computing', 'computer']\n",
        "    commonWords = 3\n",
        "    result = predPap.predict(inputKey, commonWords)\n",
        "    print(result)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{911: [4, 'network security'], 902: [4, 'distributed computing'], 912: [3, 'distributed computing'], 909: [3, 'distributed computing'], 907: [3, 'distributed computing'], 905: [3, 'network security'], 900: [3, 'distributed computing'], 895: [3, 'distributed computing']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNxpD8nSDbA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}