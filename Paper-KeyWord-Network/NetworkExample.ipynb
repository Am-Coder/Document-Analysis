{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import jsonpickle \n",
    "import math\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np # linear algebra\n",
    "from itertools import combinations \n",
    "from collections import defaultdict \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from scipy import spatial\n",
    "from nltk.metrics import edit_distance\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/holmes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/holmes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/holmes/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/holmes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNet:\n",
    "     \n",
    "    def __init__(self): \n",
    "        self.STOP_WORDS = nltk.corpus.stopwords.words()\n",
    "        \n",
    "    def tokenize(self, q1, q2):\n",
    "        return word_tokenize(q1), word_tokenize(q2)\n",
    "\n",
    "\n",
    "    def posTag(self, q1, q2):\n",
    "        return nltk.pos_tag(q1), nltk.pos_tag(q2)\n",
    "\n",
    "\n",
    "    def stemmer(self, tag_q1, tag_q2):\n",
    "        stem_q1 = []\n",
    "        stem_q2 = []\n",
    "\n",
    "        for token in tag_q1:\n",
    "            stem_q1.append(stem(token))\n",
    "\n",
    "        for token in tag_q2:\n",
    "            stem_q2.append(stem(token))\n",
    "\n",
    "        return stem_q1, stem_q2\n",
    "    \n",
    "    def path(self, set1, set2):\n",
    "        return wn.path_similarity(set1, set2)\n",
    "\n",
    "\n",
    "    def wup(self, set1, set2):\n",
    "        return wn.wup_similarity(set1, set2)\n",
    "\n",
    "\n",
    "    def edit(self, word1, word2):\n",
    "        if float(edit_distance(word1, word2)) == 0.0:\n",
    "            return 0.0\n",
    "        return 1.0 / float(edit_distance(word1, word2))\n",
    "\n",
    "    def computePath(self, q1, q2):\n",
    "\n",
    "        R = np.zeros((len(q1), len(q2)))\n",
    "\n",
    "        for i in range(len(q1)):\n",
    "            for j in range(len(q2)):\n",
    "                if q1[i][1] == None or q2[j][1] == None:\n",
    "                    sim = edit(q1[i][0], q2[j][0])\n",
    "                else:\n",
    "                    sim = path(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
    "\n",
    "                if sim == None:\n",
    "                    sim = edit(q1[i][0], q2[j][0])\n",
    "\n",
    "                R[i, j] = sim\n",
    "\n",
    "        # print R\n",
    "\n",
    "        return R\n",
    "\n",
    "    def computeWup(self, q1, q2):\n",
    "        \n",
    "        R = np.zeros((len(q1), len(q2)))\n",
    "\n",
    "        for i in range(len(q1)):\n",
    "            for j in range(len(q2)):\n",
    "                if q1[i][1] == None or q2[j][1] == None:\n",
    "                    sim = edit(q1[i][0], q2[j][0])\n",
    "                else:\n",
    "                    sim = wup(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
    "\n",
    "                if sim == None:\n",
    "                    sim = edit(q1[i][0], q2[j][0])\n",
    "\n",
    "                R[i, j] = sim\n",
    "\n",
    "        # print R\n",
    "\n",
    "        return R\n",
    "\n",
    "    def overallSim(self, q1, q2, R):\n",
    "\n",
    "        sum_X = 0.0\n",
    "        sum_Y = 0.0\n",
    "\n",
    "        for i in range(len(q1)):\n",
    "            max_i = 0.0\n",
    "            for j in range(len(q2)):\n",
    "                if R[i, j] > max_i:\n",
    "                    max_i = R[i, j]\n",
    "            sum_X += max_i\n",
    "\n",
    "        for i in range(len(q1)):\n",
    "            max_j = 0.0\n",
    "            for j in range(len(q2)):\n",
    "                if R[i, j] > max_j:\n",
    "                    max_j = R[i, j]\n",
    "            sum_Y += max_j\n",
    "\n",
    "        if (float(len(q1)) + float(len(q2))) == 0.0:\n",
    "            return 0.0\n",
    "\n",
    "        overall = (sum_X + sum_Y) / (2 * (float(len(q1)) + float(len(q2))))\n",
    "\n",
    "        return overall\n",
    "\n",
    "    def clean_sentence(self, val):\n",
    "        \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "        regex = re.compile('([^\\s\\w]|_)+')\n",
    "        sentence = regex.sub('', val).lower()\n",
    "        sentence = sentence.split(\" \")\n",
    "\n",
    "        for word in list(sentence):\n",
    "            if word in self.STOP_WORDS:\n",
    "                sentence.remove(word)\n",
    "\n",
    "        sentence = \" \".join(sentence)\n",
    "        return sentence\n",
    "    \n",
    "    def semanticSimilarity(self, q1, q2):\n",
    "        tokens_q1, tokens_q2 = self.tokenize(q1, q2)\n",
    "        # stem_q1, stem_q2 = stemmer(tokens_q1, tokens_q2)\n",
    "        tag_q1, tag_q2 = self.posTag(tokens_q1, tokens_q2)\n",
    "\n",
    "        sentence = []\n",
    "        for i, word in enumerate(tag_q1):\n",
    "            if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
    "                sentence.append(word[0])\n",
    "\n",
    "        sense1 = Lesk(sentence)\n",
    "        sentence1Means = []\n",
    "        for word in sentence:\n",
    "            sentence1Means.append(sense1.lesk(word, sentence))\n",
    "\n",
    "        sentence = []\n",
    "        for i, word in enumerate(tag_q2):\n",
    "            if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
    "                sentence.append(word[0])\n",
    "\n",
    "        sense2 = Lesk(sentence)\n",
    "        sentence2Means = []\n",
    "        for word in sentence:\n",
    "            sentence2Means.append(sense2.lesk(word, sentence))\n",
    "        # for i, word in enumerate(sentence1Means):\n",
    "        #     print sentence1Means[i][0], sentence2Means[i][0]\n",
    "\n",
    "        R1 = self.computePath(sentence1Means, sentence2Means)\n",
    "        R2 = self.computeWup(sentence1Means, sentence2Means)\n",
    "\n",
    "        R = (R1 + R2) / 2\n",
    "\n",
    "        # print R\n",
    "\n",
    "        return self.overallSim(sentence1Means, sentence2Means, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lesk(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.meanings = {}\n",
    "        for word in sentence:\n",
    "            self.meanings[word] = ''\n",
    "\n",
    "    def getSenses(self, word):\n",
    "        # print word\n",
    "        return wn.synsets(word.lower())\n",
    "\n",
    "    def getGloss(self, senses):\n",
    "\n",
    "        gloss = {}\n",
    "\n",
    "        for sense in senses:\n",
    "            gloss[sense.name()] = []\n",
    "\n",
    "        for sense in senses:\n",
    "            gloss[sense.name()] += word_tokenize(sense.definition())\n",
    "\n",
    "        return gloss\n",
    "\n",
    "    def getAll(self, word):\n",
    "        senses = self.getSenses(word)\n",
    "\n",
    "        if senses == []:\n",
    "            return {word.lower(): senses}\n",
    "\n",
    "        return self.getGloss(senses)\n",
    "\n",
    "    def Score(self, set1, set2):\n",
    "        # Base\n",
    "        overlap = 0\n",
    "\n",
    "        # Step\n",
    "        for word in set1:\n",
    "            if word in set2:\n",
    "                overlap += 1\n",
    "\n",
    "        return overlap\n",
    "\n",
    "    def overlapScore(self, word1, word2):\n",
    "\n",
    "        gloss_set1 = self.getAll(word1)\n",
    "        if self.meanings[word2] == '':\n",
    "            gloss_set2 = self.getAll(word2)\n",
    "        else:\n",
    "            # print 'here'\n",
    "            gloss_set2 = self.getGloss([wn.synset(self.meanings[word2])])\n",
    "\n",
    "        # print gloss_set2\n",
    "\n",
    "        score = {}\n",
    "        for i in gloss_set1.keys():\n",
    "            score[i] = 0\n",
    "            for j in gloss_set2.keys():\n",
    "                score[i] += self.Score(gloss_set1[i], gloss_set2[j])\n",
    "\n",
    "        bestSense = None\n",
    "        max_score = 0\n",
    "        for i in gloss_set1.keys():\n",
    "            if score[i] > max_score:\n",
    "                max_score = score[i]\n",
    "                bestSense = i\n",
    "\n",
    "        return bestSense, max_score\n",
    "\n",
    "    def lesk(self, word, sentence):\n",
    "        maxOverlap = 0\n",
    "        context = sentence\n",
    "        word_sense = []\n",
    "        meaning = {}\n",
    "\n",
    "        senses = self.getSenses(word)\n",
    "\n",
    "        for sense in senses:\n",
    "            meaning[sense.name()] = 0\n",
    "\n",
    "        for word_context in context:\n",
    "            if not word == word_context:\n",
    "                score = self.overlapScore(word, word_context)\n",
    "                if score[0] == None:\n",
    "                    continue\n",
    "                meaning[score[0]] += score[1]\n",
    "\n",
    "        if senses == []:\n",
    "            return word, None, None\n",
    "\n",
    "        self.meanings[word] = max(meaning.keys(), key=lambda x: meaning[x])\n",
    "\n",
    "        return word, self.meanings[word], wn.synset(self.meanings[word]).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node structure for graph\n",
    "class Node:\n",
    "\n",
    "    def __init__(self,src,dest,wt):\n",
    "        self.src = src\n",
    "        self.dest = dest\n",
    "        self.wt = wt\n",
    "\n",
    "\n",
    "#Class to represent an un-directed graph using adjacency list representation \n",
    "class Graph: \n",
    "   \n",
    "    def __init__(self,vertices): \n",
    "        self.V = vertices #No. of vertices \n",
    "        self.V_org = vertices \n",
    "        self.graph = defaultdict(list) # default dictionary to store graph \n",
    "        \n",
    "        \n",
    "    # function to add an edge to graph \n",
    "    def addEdge(self,u,v,w): \n",
    "        self.graph[u].append(Node(u,v,w))\n",
    "        self.graph[v].append(Node(v,u,w))\n",
    "\n",
    "        \n",
    "    #function to print graph\n",
    "    def printGraph(self):\n",
    "        s = \"\"\n",
    "        for i in self.graph:\n",
    "            s = s + str(i) + \" is connected to \"\n",
    "            print(str(i) + \" is connected to \")\n",
    "            for node in self.graph[i]:\n",
    "                s = s + str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \"\n",
    "                print(str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \")\n",
    "            s = s + \"\\n\"\n",
    "            print(\"\\n\")\n",
    "        return s\n",
    "\n",
    "    def BFSi(self, s, max_levels):\n",
    "        visited = set()\n",
    "         \n",
    "        queue = []\n",
    "        wordNet = WordNet()\n",
    "        queue.append((s,0,0,1))\n",
    "        visited.add(s)\n",
    "        level = 0\n",
    "        result = {}\n",
    "        while queue:\n",
    "            aux = []\n",
    "            result[level] = []\n",
    "            \n",
    "            while queue:\n",
    "                s = queue.pop(0)\n",
    "                visited.add(s[0])\n",
    "                result[level].append(s)\n",
    "                for node in self.graph[s[0]]:\n",
    "                    if node.dest not in visited:\n",
    "                        \n",
    "#                         Wordnet Similarity\n",
    "                        q1 = wordNet.clean_sentence(s[0])\n",
    "                        q2 = wordNet.clean_sentence(node.dest)\n",
    "                        sim = 0\n",
    "                        sim = wordNet.semanticSimilarity(q1, q2)\n",
    "\n",
    "                        sumOfCooccurence = 0\n",
    "                        for chi in self.graph[node.dest]:\n",
    "                            if chi.dest in visited:\n",
    "                                sumOfCooccurence += chi.wt\n",
    "                        aux.append((node.dest, sumOfCooccurence, level+1, sim))        \n",
    "                        visited.add(node.dest)\n",
    "            level += 1\n",
    "            if level > max_levels:\n",
    "                break\n",
    "            for node in aux:\n",
    "                queue.append(node)\n",
    "            solution = []\n",
    "            for key in result:\n",
    "                for tup in result[key]:\n",
    "                    if tup[2] != 0 and (tup[1] + np.exp(tup[3]))/np.exp(tup[2]) >= 0:\n",
    "                        solution.append( ( tup[0], (tup[1] + np.exp(tup[3]))/np.exp(tup[2]) ) )\n",
    "        return solution            \n",
    "    \n",
    "    def export_network(self, filename = \"output\"):\n",
    "        filename += \".json\"\n",
    "        obj = jsonpickle.encode(self.graph)\n",
    "        with open(filename, \"w\") as outfile: \n",
    "            json.dump(obj, outfile)\n",
    "\n",
    "    def import_network(self, filename = \"output\"):\n",
    "        filename += \".json\"\n",
    "        with open(filename) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            self.graph = jsonpickle.decode(data)\n",
    "            self.V = len(self.graph)\n",
    "            self.V_org = len(self.graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 0.7357588823428847), ('c', 1.4715177646857693), ('a', 0.6766764161830634)]\n"
     ]
    }
   ],
   "source": [
    "# Create a graph SAMPLE use \n",
    "g = Graph(4) \n",
    "g.addEdge('a', 'b', 2) \n",
    "g.addEdge('a', 'c', 2) \n",
    "g.addEdge('b', 'c', 1) \n",
    "g.addEdge('b', 'd', 1) \n",
    "g.addEdge('c', 'd', 2) \n",
    "# g.printGraph()\n",
    "print(g.BFSi('d',3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.export_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(g.graph))\n",
    "print(g.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'a': [<__main__.Node object at 0x7f76983d5690>, <__main__.Node object at 0x7f76983d51d0>], 'b': [<__main__.Node object at 0x7f76983d5c10>, <__main__.Node object at 0x7f76983d5f90>, <__main__.Node object at 0x7f76983d5390>], 'c': [<__main__.Node object at 0x7f76983d5990>, <__main__.Node object at 0x7f76983d59d0>, <__main__.Node object at 0x7f76983d5710>], 'd': [<__main__.Node object at 0x7f76983d5e90>, <__main__.Node object at 0x7f76983d5a10>]})\n",
      "[('b', 0.7357588823428847), ('c', 1.4715177646857693), ('a', 0.6766764161830634)]\n"
     ]
    }
   ],
   "source": [
    "gg = Graph(100)\n",
    "gg.import_network()\n",
    "print(gg.graph)\n",
    "print(gg.BFSi('d',3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "[('b', 0.7357588823428847), ('c', 1.4715177646857693), ('a', 0.6766764161830634)]\n"
     ]
    }
   ],
   "source": [
    "print(len(gg.graph))\n",
    "print(gg.V)\n",
    "print(gg.BFSi('d',3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
