{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NetworkGenerator_Author.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssWkC053H6tk",
        "outputId": "8341d55a-baba-420d-81c5-3f97bb0a4b91"
      },
      "source": [
        "from google.colab import drive\r\n",
        "# drive.mount('/content/gdrive')\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFUWmCSqIEdL"
      },
      "source": [
        "import re\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import pandas as pd\r\n",
        "import pathlib\r\n",
        "from itertools import combinations \r\n",
        "from collections import defaultdict "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFqFJGlpIVYp"
      },
      "source": [
        "#Node structure for graph\r\n",
        "class Node:\r\n",
        "\r\n",
        "    def __init__(self,src,dest,wt):\r\n",
        "        self.src = src\r\n",
        "        self.dest = dest\r\n",
        "        self.wt = wt\r\n",
        "\r\n",
        "\r\n",
        "#Class to represent an un-directed graph using adjacency list representation \r\n",
        "class Graph: \r\n",
        "   \r\n",
        "    def __init__(self,vertices): \r\n",
        "        self.V = vertices #No. of vertices \r\n",
        "        self.V_org = vertices \r\n",
        "        self.graph = defaultdict(list) # default dictionary to store graph \r\n",
        "        \r\n",
        "        \r\n",
        "    # function to add an edge to graph \r\n",
        "    def addEdge(self,u,v,w): \r\n",
        "        self.graph[u].append(Node(u,v,w))\r\n",
        "        self.graph[v].append(Node(v,u,w))\r\n",
        "\r\n",
        "        \r\n",
        "    #function to print graph\r\n",
        "    def printGraph(self):\r\n",
        "        s = \"\"\r\n",
        "        for i in self.graph:\r\n",
        "            s = s + str(i) + \" is connected to \"\r\n",
        "            print(str(i) + \" is connected to \")\r\n",
        "            for node in self.graph[i]:\r\n",
        "                s = s + str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \"\r\n",
        "                print(str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \")\r\n",
        "            s = s + \"\\n\"\r\n",
        "            print(\"\\n\")\r\n",
        "        return s\r\n",
        "\r\n",
        "    \r\n",
        "    #function to get BFS results for a given node till the given level\r\n",
        "    def BFS(self, s, max_levels):\r\n",
        "        visited = set()\r\n",
        " \r\n",
        "        queue = []\r\n",
        " \r\n",
        "        queue.append(s)\r\n",
        "        visited.add(s)\r\n",
        "        level = 0\r\n",
        "        result = {}\r\n",
        "        while queue:\r\n",
        "            aux = []\r\n",
        "            result[level] = []\r\n",
        "            \r\n",
        "            while queue:\r\n",
        "                s = queue.pop(0)\r\n",
        "                visited.add(s)\r\n",
        "                result[level].append(s)\r\n",
        "                for node in self.graph[s]:\r\n",
        "                    if node.dest not in visited:\r\n",
        "                        aux.append(node.dest)\r\n",
        "                        visited.add(node.dest)\r\n",
        "            level += 1\r\n",
        "            if level > max_levels:\r\n",
        "                break\r\n",
        "            for node in aux:\r\n",
        "                queue.append(node)\r\n",
        "            \r\n",
        "        return result\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class DataHandler:\r\n",
        "    def __init__(self, path):\r\n",
        "        self.type = pathlib.Path(path).suffix\r\n",
        "        self.dataset_location = path\r\n",
        "        \r\n",
        "#         Use the pandas dataframe to get columns \r\n",
        "        if self.type == '.csv':\r\n",
        "#             self.df = pd.read_csv(location, nrows=100)\r\n",
        "            self.df = pd.read_csv(path)\r\n",
        "            self.df = self.df[self.df['Domain'] == \"CS \"]\r\n",
        "        elif self.type == '.xlsx':\r\n",
        "            self.df = pd.read_excel(path)\r\n",
        "            self.df = self.df[self.df['Domain'] == \"CS \"]\r\n",
        "\r\n",
        "    def get_dataframe(self):\r\n",
        "        return self.df\r\n",
        "\r\n",
        "            \r\n",
        "class keywordNodeNetGen:\r\n",
        "    \r\n",
        "    def __init__(self, path):\r\n",
        "        self.path = path\r\n",
        "        self.subsetSize = 2\r\n",
        "        self.keysPair = []\r\n",
        "        self.keywordsList = []\r\n",
        "        self.dictKeywords = {}\r\n",
        "        self.dictKeyVsPid = {}\r\n",
        "        self.idList = []\r\n",
        "        self.colName = []\r\n",
        "    #     self.wbObj = openpyxl.load_workbook(self.path)\r\n",
        "    #     self.sheetObj = self.wbObj.active    \r\n",
        "\r\n",
        "    # #function to extract list of column headers from .xlsx\r\n",
        "    # def extractListCol(self):\r\n",
        "    #     maxCol = self.sheetObj.max_column    \r\n",
        "    #     # Loop will print all columns name \r\n",
        "    #     for i in range(1, maxCol + 1): \r\n",
        "    #         cellObj = self.sheetObj.cell(row = 1, column = i) \r\n",
        "    #         self.colName.append(cellObj.value) \r\n",
        "    \r\n",
        "\r\n",
        "    # #function to extract column index from its header\r\n",
        "    # def extractColNumber(self, strIn):\r\n",
        "    #     self.extractListCol()\r\n",
        "    #     for x in self.colName:\r\n",
        "    #         if x == strIn:\r\n",
        "    #             colIndex = self.colName.index(x)+1\r\n",
        "    #     return colIndex\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    #function to extract list of keywords in all the research papers\r\n",
        "    def extractDictOfKeywords(self):\r\n",
        "        dh = DataHandler(self.path)\r\n",
        "        df = dh.get_dataframe()\r\n",
        "        # maxRow = 1000\r\n",
        "        # keyIndex = self.extractColNumber('keywords')\r\n",
        "        rowIndices = df.index.tolist()\r\n",
        "        i=0\r\n",
        "        # for rowIndex in df.index.tolist()\r\n",
        "        for keyList in df['keywords']:\r\n",
        "            if(i<1000): \r\n",
        "                self.dictKeywords[rowIndices[i]] = [keyword.strip() for keyword in re.split(\";\", keyList) if keyword]\r\n",
        "                i=i+1\r\n",
        "\r\n",
        "        return self.dictKeywords\r\n",
        "\r\n",
        "\r\n",
        "    #function to create list of keywords in all papers\r\n",
        "    def generateKeywordsList(self):\r\n",
        "        self.dictKeywords = self.extractDictOfKeywords()\r\n",
        "        # print(len(self.dictKeywords))\r\n",
        "        self.keywordsList = list(set().union(*self.dictKeywords.values()))\r\n",
        "        return self.keywordsList\r\n",
        "    \r\n",
        "    \r\n",
        "    #function to find combinations of any 2 keywords\r\n",
        "    def rSubset(self): \r\n",
        "        self.keywordsList = self.generateKeywordsList()\r\n",
        "        self.keysPair = list(combinations(self.keywordsList, self.subsetSize)) \r\n",
        "        return self.keysPair\r\n",
        "    \r\n",
        "    \r\n",
        "    #function to extract weight\r\n",
        "    def pathWeight(self, x, y, dictKeywords, idList):\r\n",
        "        edgeWeight = 0\r\n",
        "        for idNo in idList:\r\n",
        "            if(x in dictKeywords[idNo] and y in dictKeywords[idNo]):\r\n",
        "                edgeWeight = edgeWeight + 1\r\n",
        "        return edgeWeight\r\n",
        "    \r\n",
        "    \r\n",
        "    #function to generate dictionary having keywords as keys and list of paper ids as value list \r\n",
        "    def generateDictKeyVsPid(self):\r\n",
        "        self.keywordsList = self.generateKeywordsList()\r\n",
        "        self.idList = self.dictKeywords.keys()\r\n",
        "        self.dictKeyVsPid = {key:[pid for pid in self.idList if key in self.dictKeywords[pid]] for key in self.keywordsList}\r\n",
        "        return self.dictKeyVsPid"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGHBSOway44v"
      },
      "source": [
        "# Driver Function \r\n",
        "if __name__ == \"__main__\": \r\n",
        "    path = \"drive/My Drive/Colab Notebooks/Data.xlsx\"\r\n",
        "    netGen = keywordNodeNetGen(path)\r\n",
        "    dictKeyVsPid = netGen.generateDictKeyVsPid()\r\n",
        "    dictKeywords = netGen.extractDictOfKeywords()\r\n",
        "    idList = dictKeywords.keys()\r\n",
        "    nodesCount = len(netGen.generateKeywordsList())\r\n",
        "    print(nodesCount)\r\n",
        "    keyPairList = netGen.rSubset()\r\n",
        "    g = Graph(nodesCount) \r\n",
        "    for i in keyPairList:\r\n",
        "        edgeWeight = netGen.pathWeight(i[0],i[1],dictKeywords,idList)\r\n",
        "        if edgeWeight>1:\r\n",
        "            g.addEdge(i[0], i[1], edgeWeight)\r\n",
        "        \r\n",
        "        \r\n",
        "    g.printGraph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hISqCrZ4SkBZ"
      },
      "source": [
        "queryKey = 'cloud computing'\r\n",
        "queryDict = g.BFS(queryKey, 3)\r\n",
        "\r\n",
        "querySet = []\r\n",
        "querySet.append(queryKey)\r\n",
        "querySet.extend([item for sublist in queryDict.values() for item in sublist])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qszh38X3yUUA"
      },
      "source": [
        "# Gives sorted dictionary result of papers and number of matching keywords \r\n",
        "# between input keywords and rake extracted keywords from abstract\r\n",
        "def predict_rake(input_keywords, common_words_count):\r\n",
        "    res = {}\r\n",
        "    ran = len(dataset['Rake keywords'])\r\n",
        "    for i in range(ran):\r\n",
        "        val = dataset['Rake keywords'][i]\r\n",
        "        keywords = val.split(\";\")\r\n",
        "        com = calc_common(input_keywords, keywords)\r\n",
        "        if com >= common_words_count:\r\n",
        "            res[i] = com\r\n",
        "    res = sorted(res.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\r\n",
        "    return res\r\n",
        "\r\n",
        "# Gives sorted dictionary result of papers and number of matching keywords \r\n",
        "# between input keywords and yake extracted keywords from abstract\r\n",
        "def predict_yake(input_keywords, common_words_count):\r\n",
        "    res = {}\r\n",
        "    ran = len(dataset['Yake keywords'])\r\n",
        "    for i in range(ran):\r\n",
        "        val = dataset['Yake keywords'][i]\r\n",
        "        keywords = val.split(\";\")\r\n",
        "        com = calc_common(input_keywords, keywords)\r\n",
        "        if com >= common_words_count:\r\n",
        "            res[i] = com\r\n",
        "    res = sorted(res.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\r\n",
        "    return res\r\n",
        "\r\n",
        "# Gives sorted dictionary result of papers and number of matching keywords \r\n",
        "# between input keywords and author labelled keywords from abstract\r\n",
        "def predict_author_labelled(input_keywords, common_words_count):\r\n",
        "    res = {}\r\n",
        "    ran = len(dataset['keywords'])\r\n",
        "    for i in range(ran):\r\n",
        "        val = dataset['keywords'][i]\r\n",
        "        keywords = val.split(\";\")\r\n",
        "        com = calc_common(input_keywords, keywords)\r\n",
        "        if com >= common_words_count:\r\n",
        "            res[i] = com\r\n",
        "    res = sorted(res.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\r\n",
        "    return res\r\n",
        "\r\n",
        "def calc_common(input_keywords, keywords):\r\n",
        "    cou = 0\r\n",
        "    for w1 in input_keywords:\r\n",
        "        for w2 in keywords:\r\n",
        "            if(w1 == w2):\r\n",
        "                cou+=1\r\n",
        "    return cou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msnV2V0oyVSo"
      },
      "source": [
        "result = predict_author_labelled(querySet, 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}