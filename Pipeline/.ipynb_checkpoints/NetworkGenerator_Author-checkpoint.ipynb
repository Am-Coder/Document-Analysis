{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssWkC053H6tk",
    "outputId": "8341d55a-baba-420d-81c5-3f97bb0a4b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rFUWmCSqIEdL"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from itertools import combinations \n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yFqFJGlpIVYp"
   },
   "outputs": [],
   "source": [
    "#Node structure for graph\n",
    "class Node:\n",
    "\n",
    "    def __init__(self,src,dest,wt):\n",
    "        self.src = src\n",
    "        self.dest = dest\n",
    "        self.wt = wt\n",
    "\n",
    "\n",
    "#Class to represent an un-directed graph using adjacency list representation \n",
    "class Graph: \n",
    "   \n",
    "    def __init__(self,vertices): \n",
    "        self.V = vertices #No. of vertices \n",
    "        self.V_org = vertices \n",
    "        self.graph = defaultdict(list) # default dictionary to store graph \n",
    "        \n",
    "        \n",
    "    # function to add an edge to graph \n",
    "    def addEdge(self,u,v,w): \n",
    "        self.graph[u].append(Node(u,v,w))\n",
    "        self.graph[v].append(Node(v,u,w))\n",
    "\n",
    "        \n",
    "    #function to print graph\n",
    "    def printGraph(self):\n",
    "        s = \"\"\n",
    "        for i in self.graph:\n",
    "            s = s + str(i) + \" is connected to \"\n",
    "            print(str(i) + \" is connected to \")\n",
    "            for node in self.graph[i]:\n",
    "                s = s + str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \"\n",
    "                print(str(node.dest) + \"(Weight = \" + str(node.wt) + \")\" + \" \")\n",
    "            s = s + \"\\n\"\n",
    "            print(\"\\n\")\n",
    "        return s\n",
    "\n",
    "    \n",
    "    #function to get BFS results for a given node till the given level\n",
    "    def BFS(self, s, max_levels):\n",
    "        visited = set()\n",
    " \n",
    "        queue = []\n",
    " \n",
    "        queue.append(s)\n",
    "        visited.add(s)\n",
    "        level = 0\n",
    "        result = {}\n",
    "        while queue:\n",
    "            aux = []\n",
    "            result[level] = []\n",
    "            \n",
    "            while queue:\n",
    "                s = queue.pop(0)\n",
    "                visited.add(s)\n",
    "                result[level].append(s)\n",
    "                for node in self.graph[s]:\n",
    "                    if node.dest not in visited:\n",
    "                        aux.append(node.dest)\n",
    "                        visited.add(node.dest)\n",
    "            level += 1\n",
    "            if level > max_levels:\n",
    "                break\n",
    "            for node in aux:\n",
    "                queue.append(node)\n",
    "            \n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, path):\n",
    "        self.type = pathlib.Path(path).suffix\n",
    "        self.dataset_location = path\n",
    "        \n",
    "#         Use the pandas dataframe to get columns \n",
    "        if self.type == '.csv':\n",
    "#             self.df = pd.read_csv(location, nrows=100)\n",
    "            self.df = pd.read_csv(path)\n",
    "            self.df = self.df[self.df['Domain'] == \"CS \"]\n",
    "        elif self.type == '.xlsx':\n",
    "            self.df = pd.read_excel(path)\n",
    "            self.df = self.df[self.df['Domain'] == \"CS \"]\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        return self.df\n",
    "\n",
    "            \n",
    "class keywordNodeNetGen:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.subsetSize = 2\n",
    "        self.keysPair = []\n",
    "        self.keywordsList = []\n",
    "        self.dictKeywords = {}\n",
    "        self.dictKeyVsPid = {}\n",
    "        self.idList = []\n",
    "        self.colName = []\n",
    "    #     self.wbObj = openpyxl.load_workbook(self.path)\n",
    "    #     self.sheetObj = self.wbObj.active    \n",
    "\n",
    "    # #function to extract list of column headers from .xlsx\n",
    "    # def extractListCol(self):\n",
    "    #     maxCol = self.sheetObj.max_column    \n",
    "    #     # Loop will print all columns name \n",
    "    #     for i in range(1, maxCol + 1): \n",
    "    #         cellObj = self.sheetObj.cell(row = 1, column = i) \n",
    "    #         self.colName.append(cellObj.value) \n",
    "    \n",
    "\n",
    "    # #function to extract column index from its header\n",
    "    # def extractColNumber(self, strIn):\n",
    "    #     self.extractListCol()\n",
    "    #     for x in self.colName:\n",
    "    #         if x == strIn:\n",
    "    #             colIndex = self.colName.index(x)+1\n",
    "    #     return colIndex\n",
    "\n",
    "\n",
    "\n",
    "    #function to extract list of keywords in all the research papers\n",
    "    def extractDictOfKeywords(self):\n",
    "        dh = DataHandler(self.path)\n",
    "        df = dh.get_dataframe()\n",
    "        # maxRow = 1000\n",
    "        # keyIndex = self.extractColNumber('keywords')\n",
    "        rowIndices = df.index.tolist()\n",
    "        i=0\n",
    "        # for rowIndex in df.index.tolist()\n",
    "        for keyList in df['keywords']:\n",
    "            if(i<1000): \n",
    "                self.dictKeywords[rowIndices[i]] = [keyword.strip() for keyword in re.split(\";\", keyList) if keyword]\n",
    "                i=i+1\n",
    "\n",
    "        return self.dictKeywords\n",
    "\n",
    "\n",
    "    #function to create list of keywords in all papers\n",
    "    def generateKeywordsList(self):\n",
    "        self.dictKeywords = self.extractDictOfKeywords()\n",
    "        # print(len(self.dictKeywords))\n",
    "        self.keywordsList = list(set().union(*self.dictKeywords.values()))\n",
    "        return self.keywordsList\n",
    "    \n",
    "    \n",
    "    #function to find combinations of any 2 keywords\n",
    "    def rSubset(self): \n",
    "        self.keywordsList = self.generateKeywordsList()\n",
    "        self.keysPair = list(combinations(self.keywordsList, self.subsetSize)) \n",
    "        return self.keysPair\n",
    "    \n",
    "    \n",
    "    #function to extract weight\n",
    "    def pathWeight(self, x, y, dictKeywords, idList):\n",
    "        edgeWeight = 0\n",
    "        for idNo in idList:\n",
    "            if(x in dictKeywords[idNo] and y in dictKeywords[idNo]):\n",
    "                edgeWeight = edgeWeight + 1\n",
    "        return edgeWeight\n",
    "    \n",
    "    \n",
    "    #function to generate dictionary having keywords as keys and list of paper ids as value list \n",
    "    def generateDictKeyVsPid(self):\n",
    "        self.keywordsList = self.generateKeywordsList()\n",
    "        self.idList = self.dictKeywords.keys()\n",
    "        self.dictKeyVsPid = {key:[pid for pid in self.idList if key in self.dictKeywords[pid]] for key in self.keywordsList}\n",
    "        return self.dictKeyVsPid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGHBSOway44v"
   },
   "outputs": [],
   "source": [
    "# Driver Function \n",
    "if __name__ == \"__main__\": \n",
    "    path = \"drive/My Drive/Colab Notebooks/Data.xlsx\"\n",
    "    netGen = keywordNodeNetGen(path)\n",
    "    dictKeyVsPid = netGen.generateDictKeyVsPid()\n",
    "    dictKeywords = netGen.extractDictOfKeywords()\n",
    "    idList = dictKeywords.keys()\n",
    "    nodesCount = len(netGen.generateKeywordsList())\n",
    "    print(nodesCount)\n",
    "    keyPairList = netGen.rSubset()\n",
    "    g = Graph(nodesCount) \n",
    "    for i in keyPairList:\n",
    "        edgeWeight = netGen.pathWeight(i[0],i[1],dictKeywords,idList)\n",
    "        if edgeWeight>1:\n",
    "            g.addEdge(i[0], i[1], edgeWeight)\n",
    "        \n",
    "        \n",
    "    g.printGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hISqCrZ4SkBZ"
   },
   "outputs": [],
   "source": [
    "queryKey = 'cloud computing'\n",
    "queryDict = g.BFS(queryKey, 3)\n",
    "\n",
    "querySet = []\n",
    "querySet.append(queryKey)\n",
    "querySet.extend([item for sublist in queryDict.values() for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qszh38X3yUUA"
   },
   "outputs": [],
   "source": [
    "# Gives sorted dictionary result of papers and number of matching keywords \n",
    "# between input keywords and rake extracted keywords from abstract\n",
    "def predict_rake(input_keywords, common_words_count):\n",
    "    res = {}\n",
    "    ran = len(dataset['Rake keywords'])\n",
    "    for i in range(ran):\n",
    "        val = dataset['Rake keywords'][i]\n",
    "        keywords = val.split(\";\")\n",
    "        com = calc_common(input_keywords, keywords)\n",
    "        if com >= common_words_count:\n",
    "            res[i] = com\n",
    "    res = sorted(res.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
    "    return res\n",
    "\n",
    "# Gives sorted dictionary result of papers and number of matching keywords \n",
    "# between input keywords and yake extracted keywords from abstract\n",
    "def predict_yake(input_keywords, common_words_count):\n",
    "    res = {}\n",
    "    ran = len(dataset['Yake keywords'])\n",
    "    for i in range(ran):\n",
    "        val = dataset['Yake keywords'][i]\n",
    "        keywords = val.split(\";\")\n",
    "        com = calc_common(input_keywords, keywords)\n",
    "        if com >= common_words_count:\n",
    "            res[i] = com\n",
    "    res = sorted(res.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
    "    return res\n",
    "\n",
    "# Gives sorted dictionary result of papers and number of matching keywords \n",
    "# between input keywords and author labelled keywords from abstract\n",
    "def predict_author_labelled(input_keywords, common_words_count):\n",
    "    res = {}\n",
    "    ran = len(dataset['keywords'])\n",
    "    for i in range(ran):\n",
    "        val = dataset['keywords'][i]\n",
    "        keywords = val.split(\";\")\n",
    "        com = calc_common(input_keywords, keywords)\n",
    "        if com >= common_words_count:\n",
    "            res[i] = com\n",
    "    res = sorted(res.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
    "    return res\n",
    "\n",
    "def calc_common(input_keywords, keywords):\n",
    "    cou = 0\n",
    "    for w1 in input_keywords:\n",
    "        for w2 in keywords:\n",
    "            if(w1 == w2):\n",
    "                cou+=1\n",
    "    return cou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msnV2V0oyVSo"
   },
   "outputs": [],
   "source": [
    "result = predict_author_labelled(querySet, 1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NetworkGenerator_Author.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
